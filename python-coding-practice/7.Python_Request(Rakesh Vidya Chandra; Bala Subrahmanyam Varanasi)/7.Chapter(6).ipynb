{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Chapter 6. Web Scraping with Python Requests and BeautifulSoup\n",
    "We have become experts in how to communicate with the Web through Requests. Everything progressed flamboyantly while working with the APIs. However, there are some conditions where we need to be aware of API folklore.\n",
    "\n",
    "The first thing that concerns us is not all web services have built an API for the sake of their third-party customers. Also, there is no statute that the API should be maintained perfectly. Even tech giants such as Google, Facebook, and Twitter tend to change their APIs abruptly without prior notice. So, it's better to understand that it is not always the API that comes to the rescue when we are looking for some vital information from a web resource.\n",
    "\n",
    "The concept of web scraping stands as a savior when we really turn imperative to access some information from a web resource that does not maintain an API. In this chapter, we will discuss tricks of the trade to extract information from web resources by following all the principles of web scraping.\n",
    "\n",
    "Before we begin, let's get to know some important concepts that will help us to reach our goal. Take a look at the response content format of a request, which will introduce us to a particular type of data:\n",
    "\n",
    ">>> import requests\n",
    ">>> r = requests.get(\"http://en.wikipedia.org/wiki/List_of_algorithms\")\n",
    ">>> r\n",
    "<Response [200]>\n",
    ">>> r.text\n",
    "u'<!DOCTYPE html>\\n<html lang=\"en\" dir=\"ltr\" class=\"client-nojs\">\\n<head>\\n<meta charset=\"UTF-8\" />\\n<title>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In the preceding example, the response content is rendered in the form of semistructured data, which is \n",
    "represented using HTML tags; this in turn helps us to access the information about the different sections of a web\n",
    "page individually.\n",
    "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "Now, let's get to know the different types of data that the Web generally deals with.\n",
    "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "Types of data\n",
    "In most cases, we deal with three types of data when working with web sources. They are as follows:\n",
    "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "Structured data\n",
    "Unstructured data\n",
    "Semistructured Data\n",
    "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Structured data\n",
    "Structured data is a type of data that exists in an organized form. Normally, structured data has a predefined \n",
    "format and it is machine readable. Each piece of data that lies in structured data has a relation with every other \n",
    "data as a specific format is imposed on it. This makes it easier and faster to access different parts of data. \n",
    "The structured data type helps in mitigating redundant data while dealing with huge amounts of data.\n",
    "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "Databases always contain structured data, and SQL techniques can be used to access data from them. We can regard census records as an example of structured data. They contain information about the date of birth, gender, place, income, and so on, of the people of a country.\n",
    "\n",
    "Unstructured data\n",
    "In contrast to structured data, unstructured data either misses out on a standard format or stays unorganized even though a specific format is imposed on it. Due to this reason, it becomes difficult to deal with different parts of the data. Also, it turns into a tedious task. To handle unstructured data, different techniques such as text analytics, Natural Language Processing (NLP), and data mining are used. Images, scientific data, text-heavy content (such as newspapers, health records, and so on), come under the unstructured data type.\n",
    "\n",
    "Semistructured data\n",
    "Semistructured data is a type of data that follows an irregular trend or has a structure which changes rapidly. This data can be a self described one, it uses tags and other markers to establish a semantic relationship among the elements of the data. Semistructured data may contain information that is transferred from different sources. Scraping is the technique that is used to extract information from this type of data. The information available on the Web is a perfect example of semistructured data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "What is web scraping?\n",
    "In simple words, web scraping is the process of extracting desired data from a web resource. This method involves \n",
    "different procedures such as interacting with the web resource, choosing the appropriate data, obtaining \n",
    "information from the data, and converting the data to the desired format. With all the previous methods \n",
    "considered, a major spotlight will be thrown on the process of pulling the required data from the semistructured \n",
    "data.\n",
    "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "Dos and don'ts of web scraping\n",
    "Scraping a web resource is not always welcomed by the owners. Some companies put a restriction on using bots against them. It's etiquette to follow certain rules while scraping. The following are the dos and don'ts of web scraping:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Do refer to the terms and conditions: The first thing that should come to our mind before we begin scraping is terms and conditions. Do visit the website's terms and conditions page and get to know whether they prohibit scraping from their site. If so, it's better to back off.\n",
    "Don't bombard the server with a lot of requests: Every website runs on a server that can serve only a specific amount of workload. It is equivalent to being rude if we bombard the server with lots of requests in a specific span of time, which may result in sever breakdown. Wait for some time between requests instead of bombarding the server with too many requests at once.\n",
    "NOTE\n",
    "Some sites put a restriction on the maximum number of requests processed per minute and will ban the request sender's IP address if this is not adhered to.\n",
    "\n",
    "Do track the web resource from time to time: A website doesn't always stay the same. According to its usability and the requirement of users, they tend to change from time to time. If any alteration has taken place in the website, our code to scrape may fail. Do remember to track the changes made to the site, modify the scrapper script, and scrape accordingly.\n",
    "Predominant steps to perform web scraping\n",
    "Generally, the process of web scraping requires the use of different tools and libraries such as the following:\n",
    "\n",
    "Chrome DevTools or FireBug Add-on: This can be used to pinpoint the pieces of information in an HTML/XML page.\n",
    "HTTP libraries: These can be used to interact with the server and to pull a response document. An example of this is python-requests.\n",
    "Web scraping tools: These are used to pull data from a semistructured document. Examples include BeautifulSoup or Scrappy.\n",
    "The overall picture of web scraping can be observed in the following steps:\n",
    "\n",
    "Identify the URL(s) of the web resource to perform the web scraping task.\n",
    "Use your favorite HTTP client/library to pull the semistructured document.\n",
    "Before extracting the desired data, discover the pieces of data that are in semistructured format.\n",
    "Utilize a web scraping tool to parse the acquired semistructured document into a more structured one.\n",
    "Draw the desired data that we are hoping to use. That's all, we are done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Key web scraping tasks\n",
    "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "While pulling the required data from a semistructured document, we perform various tasks. The following are the \n",
    "basic tasks that we adopt for scraping:\n",
    "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "Searching a semistructured document: Accessing a particular element or a specific type of element in a document \n",
    "can be accomplished using its tag name and tag attributes, such as id, class, and so on.\n",
    "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "Navigating within a semistructured document: We can navigate through a web document to pull different types of \n",
    "data in four ways, which are navigating down, navigating sideways, navigating up, and navigating back and forth.\n",
    "We can get to know more about these in detail later in this chapter.Modifying a semistructured document:\n",
    "By modifying the tag name or the tag attributes of a document, we can streamline and pull the required data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "What is BeautifulSoup?\n",
    "The BeautifulSoup library is a simple yet powerful web scraping library. It has the capability to extract the desired data when provided with an HTML or XML document. It is charged with some superb methods, which help us to perform web scraping tasks effortlessly.\n",
    "\n",
    "Document parsers\n",
    "Document parsers aid us in parsing and serializing the semistructured documents that are written using HTML5, lxml, or any other markup language. By default, BeautifulSoup has Python's standard HTMLParser object. If we are dealing with different types of documents, such as HTML5 and lxml, we need to install them explicitly.\n",
    "\n",
    "In this chapter, our prime focus will be laid only on particular parts of the library, which help us to understand the techniques to develop a practical scraping bot that we will build at the end of this chapter.\n",
    "\n",
    "Installation\n",
    "Installing BeautifulSoup is pretty straightforward. We can use pip to install it with ease:\n",
    "\n",
    "$ pip install beautifulsoup4\n",
    "Whenever we intend to scrape a web resource using BeautifulSoup, we need to create a BeautifulSoup object for it. The following are the commands to do this:\n",
    "\n",
    ">>> from bs4 import BeautifulSoup\n",
    ">>> soup = BeautifulSoup(<HTML_DOCUMENT_STRING>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Objects in BeautifulSoup\n",
    "The BeautifulSoup object parses the given HTML/XML document and converts it into a tree of Python objects, which are discussed in the following sections.\n",
    "\n",
    "TAGS\n",
    "The word \"tag\" represents an HTML/XML tag in the provided document. Each tag object has a name and a lot of attributes and methods. The following example showcases the way to deal with a tag object:\n",
    "\n",
    ">>> from bs4 import BeautifulSoup\n",
    ">>> soup = BeautifulSoup(\"<h1 id='message'>Hello, Requests!</h1>\")\n",
    "In order to access the type, name, and attributes of the BeautifulSoup object, with soup, that we created in the preceding example, use the following commands:\n",
    "\n",
    "For accessing the tag type:\n",
    ">>> tag = soup.h1\n",
    ">>> type(tag)\n",
    "<class 'bs4.element.Tag'>\n",
    "For accessing the tag name:\n",
    ">>> tag.name\n",
    "'h1'\n",
    "For accessing the tag attribute ('id' in the given html string)\n",
    ">>> tag['id']\n",
    "'message'\n",
    "BEAUTIFULSOUP\n",
    "The object that gets created when we intend to scrape a web resource is called a BeautifulSoup object. Put simply, it is the complete document that we are planning to scrape. This can be done using the following commands:\n",
    "\n",
    ">>> from bs4 import BeautifulSoup\n",
    ">>> soup = BeautifulSoup(\"<h1 id='message'>Hello, Requests!</h1>\") >>> type(soup)\n",
    "<class 'bs4.BeautifulSoup'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAVIGABLESTRING\n",
    "A NavigableString object represents the contents of tag. We use the .string attribute of the tag object to access it:\n",
    "\n",
    ">>> tag.string\n",
    "u'Hello, Requests!'\n",
    "COMMENTS\n",
    "The comment object illustrates the comment part of the web document. The following lines of code exemplify a comment object:\n",
    "\n",
    ">>> soup = BeautifulSoup(\"<p><!-- This is comment --></p>\")\n",
    ">>> comment = soup.p.string\n",
    ">>> type(comment)\n",
    "<class 'bs4.element.Comment'>\n",
    "Web scraping tasks related to BeautifulSoup\n",
    "As cited in the previous section of Key web scraping tasks, BeautifulSoup always follows those basic tasks in the process of web scraping. We can get to know these tasks in detail with the help of a practical example, using an HTML document. We will be using the following HTML document that is scraping_example.html, as an example through out the chapter:\n",
    "\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "  <head>\n",
    "    <meta charset=\"UTF-8\" />\n",
    "    <title>\n",
    "      Chapter 6 - Web Scrapping with Python Requests and BeatuifulSoup\n",
    "    </title>\n",
    "  </head>\n",
    "  <body>\n",
    "    <div class=\"surveys\">\n",
    "      <div class=\"survey\" id=\"1\">\n",
    "        <p class=\"question\">\n",
    "          <a href=\"/surveys/1\">Are you from India?</a>\n",
    "        </p>\n",
    "        <ul class=\"responses\">\n",
    "          <li class=\"response\">Yes - <span class=\"score\">21</span>\n",
    "          </li>\n",
    "          <li class=\"response\">No - <span class=\"score\">19</span>\n",
    "          </li>\n",
    "        </ul>\n",
    "      </div>\n",
    "      <div class=\"survey\" id=\"2\">\n",
    "        <p class=\"question\">\n",
    "          <a href=\"/surveys/2\">Have you ever seen the rain?</a>\n",
    "        </p>\n",
    "        <ul class=\"responses\">\n",
    "          <li class=\"response\">Yes - <span class=\"score\">40</span>\n",
    "          </li>\n",
    "          <li class=\"response\">No - <span class=\"score\">0</span>\n",
    "          </li>\n",
    "        </ul>\n",
    "      </div>\n",
    "      <div class=\"survey\" id=\"3\">\n",
    "        <p class=\"question\">\n",
    "          <a href=\"/surveys/1\">Do you like grapes?</a>\n",
    "        </p>\n",
    "        <ul class=\"responses\">\n",
    "          <li class=\"response\">Yes - <span class=\"score\">34</span>\n",
    "          </li>\n",
    "          <li class=\"response\">No - <span class=\"score\">6</span>\n",
    "          </li>\n",
    "        </ul>\n",
    "      </div>\n",
    "    </div>\n",
    "  </body>\n",
    "</html>\n",
    "To give a crystal clear understanding of the preceding web document, we showcased it as a document tree. The following diagram represents the preceding HTML document:\n",
    "\n",
    "Web scraping tasks related to BeautifulSoup\n",
    "When we create the BeautifulSoup object for the previously shown web document, it will result in a tree of Python objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "To perform different tasks with the previous document, scraping_example.html, we need to create a BeautifulSoup object. To create it, open the Python shell and run the following commands:\n",
    "\n",
    ">>> from bs4 import BeautifulSoup\n",
    ">>> soup = BeautifulSoup(open(\"scraping_example.html\"))\n",
    "From now, we will use the preceding BeautifulSoup object to execute different tasks. Let's perform the web scraping tasks on the scraping_example.html document and get an overall idea on all the tasks.\n",
    "\n",
    "Searching the tree\n",
    "To identify the different tags in an HTML/XML document, we need to search the whole document. In similar situations, we can use BeautifulSoup methods such as find, find_all, and so on.\n",
    "\n",
    "Here is the syntax to search the whole document to identify the tags:\n",
    "\n",
    "find(name, attributes, recursive, text, **kwargs)\n",
    "name: This is the first occurring tag name that appears in the process of discovery. It can be a string, a regular expression, a list, a function, or the value True.\n",
    "find_all(name, attributes, recursive, text, limit, **kwargs)\n",
    "name: This is used to access specific types of tags with their name. It can be a string, a regular expression, a list, a function, or the value True.\n",
    "limit: This is the maximum number of results in the output.\n",
    "The common attributes for the preceding two methods are as follows:\n",
    "\n",
    "attributes: These are the attributes of an HTML/XML tag.\n",
    "recursive: This takes a Boolean value. If it is set to True, the BeautifulSoup library checks all the children of a specific tag. Vice versa, if it is set to false, the BeautifulSoup library checks the child at the next level only.\n",
    "text: This parameter identifies tags that consist of the string content.\n",
    "NAVIGATING WITHIN THE TREE\n",
    "Different tasks are involved in navigating the document tree with the Beautifulsoup4 module; they are discussed in the following section.\n",
    "\n",
    "Navigating down\n",
    "We can access a particular element's data by moving down in a document. If we consider the document tree in the previous figure, we can access different elements by moving downward from the top element—html.\n",
    "\n",
    "Every element can be accessed using its tag name. Here is a way to access the contents of the html attribute:\n",
    "\n",
    ">>> soup.html\n",
    "<html lang=\"en\">\n",
    "...\n",
    "...\n",
    "</html>\n",
    "Here are the ways in which we can access the elements of the preceding document tree by navigating down. In order to access the title element, we should go from top to bottom, that is, from html to head and from head to title, as shown in the following command:\n",
    "\n",
    ">>> soup.html.head.title\n",
    "<title>Chapter 6 - Web Scraping with Python Requests and BeatuifulSoup</title>\n",
    "Similarly, you can access the meta element, as shown in the following command:\n",
    "\n",
    ">>> soup.html.head.meta\n",
    "<meta charset=\"utf-8\"/>\n",
    "Navigating sideways\n",
    "To access the siblings in a document tree, we should navigate sideways. The BeautifulSoup library provides various tag object properties such as .next_sibling, .previous_sibling, .next_siblings, and .previous_siblings.\n",
    "\n",
    "If you look at the preceding diagram containing the document tree, the different siblings at different levels of the tree, when navigated sideways, are as follows:\n",
    "\n",
    "head and body\n",
    "div1, div2, and div3\n",
    "In the document tree, the head tag is the first child of html, and body is the next child of html. In order to access the children of the html tag, we can use its children property:\n",
    "\n",
    ">>> for child in soup.html.children:\n",
    "...     print child.name\n",
    "...\n",
    "head\n",
    "body\n",
    "To access the next sibling of head element we can use .find_next_sibling:\n",
    "\n",
    ">>> soup.head.find_next_sibling()\n",
    "<body>\n",
    "    <div class=\"surveys\">\n",
    "        .\n",
    "        .\n",
    "        .\n",
    "    </div>\n",
    "</body>\n",
    "To access the previous sibling of body, we can use .find_previous_sibling:\n",
    "\n",
    ">>> soup.body.find_previous_sibling\n",
    "<head><meta charset=\"utf-8\"/><title>... </title></head>\n",
    "Navigating up\n",
    "We can access a particular element's parent by moving toward the top of the document tree. The BeautifulSoup library provides two properties—.parent and .parents—to access the first parent of the tag element and all its ancestors, respectively.\n",
    "\n",
    "Here is an example:\n",
    "\n",
    ">>> soup.div.parent.name\n",
    "'body'\n",
    "\n",
    ">>> for parent in soup.div.parents:\n",
    "...     print parent.name\n",
    "...\n",
    "body\n",
    "html\n",
    "[document]\n",
    "Navigating back and forth\n",
    "To access the previously parsed element, we navigate back in the node of a tree, and to access the immediate element that gets parsed next, we navigate forward in the node of a tree. To deal with this, the tag object provides the .find_previous_element and .find_next_element properties, as shown in the following example:\n",
    "\n",
    ">>> soup.head.find_previous().name\n",
    "'html'\n",
    ">>> soup.head.find_next().name\n",
    "'meta'\n",
    "Modifying the Tree\n",
    "The BeautifulSoup library also facilitates us to make changes to the web document according to our requirements. We can alter a tag's properties using its attributes, such as the .name, .string, and .append() method. We can also add new tags and strings to an existing tag with the help of the .new_string() and .new_tag() methods. There are also other methods, such as .insert(), .insert_before(), .insert_after(), and so on, to make various modifications to the document tree.\n",
    "\n",
    "Here is an example of changing the title tag's .string attribute:\n",
    "\n",
    "Before modifying the title tag the title contents are:\n",
    ">>> soup.title.string\n",
    "u'Chapter 6 - Web Scrapping with Python Requests and BeatuifulSoup'\n",
    "This is the way to modify the contents of a title tag:\n",
    ">>> soup.title.string = 'Web Scrapping with Python Requests and BeatuifulSoup by Balu and Rakhi'\n",
    "After the modifications the contents of the tilte tag looks like this:\n",
    ">>> soup.title.string\n",
    "u'Web Scrapping with Python Requests and BeatuifulSoup by Balu and Rakhi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Building a web scraping bot – a practical example\n",
    "At this point of time, our minds got enlightened with all sorts of clues to scrape the Web. With all the information acquired, let's look at a practical example. Now, we will create a web scraping bot, which will pull a list of words from a web resource and store them in a JSON file.\n",
    "\n",
    "Let's turn on the scraping mode!\n",
    "\n",
    "The web scraping bot\n",
    "Here, the web scraping bot is an automated script that has the capability to extract words from a website named majortests.com. This website consists of various tests and Graduate Record Examinations (GRE) word lists. With this web scraping bot, we will scrape the previously mentioned website and create a list of GRE words and their meanings in a JSON file.\n",
    "\n",
    "The following image is the sample page of the website that we are going to scrape:\n",
    "\n",
    "The web scraping bot\n",
    "Before we kick start the scraping process, let's revise the dos and don't of web scraping as mentioned in the initial part of the chapter. Believe it or not they will definitely leave us in peace:\n",
    "\n",
    "Do refer to the terms and conditions: Yes, before scraping majortests.com, refer to the terms and conditions of the site and obtain the necessary legal permissions to scrape it.\n",
    "Don't bombard the server with a lot of requests: Keeping this in mind, for every request that we are going to send to the website, a delay has been instilled using Python's time.sleep function.\n",
    "Do track the web resource from time to time: We ensured that the code runs perfectly with the website that is running on the server. Do check the site once before starting to scrape, so that it won't break the code. This can be made possible by running some unit tests, which conform to the structure we expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Now, let's start the implementation by following the steps to scrape that we discussed previously.\n",
    "\n",
    "IDENTIFYING THE URL OR URLS\n",
    "The first step in web scraping is to identify the URL or a list of URLs that will result in the required resources. In this case, our intent is to find all the URLs that result in the expected list of GRE words. The following is the list of the URLs of the sites that we are going to scrape:\n",
    "\n",
    "http://www.majortests.com/gre/wordlist_01,\n",
    "\n",
    "http://www.majortests.com/gre/wordlist_02,\n",
    "\n",
    "http://www.majortests.com/gre/wordlist_03, and so on\n",
    "\n",
    "Our aim is to scrape words from nine such URLs, for which we found a common pattern. This will help us to crawl all of them. The common URL pattern for all those URLs is written using Python's string object, as follows:\n",
    "\n",
    "http://www.majortests.com/gre/wordlist_0%d\n",
    "\n",
    "In our implementation, we defined a method called generate_urls, which will generate the required list of URLs using the preceding URL string. The following snippet demonstrates the process in a Python shell:\n",
    "\n",
    ">>> START_PAGE, END_PAGE = 1, 10\n",
    ">>> URL = \"http://www.majortests.com/gre/wordlist_0%d\"\n",
    ">>> def generate_urls(url, start_page, end_page):\n",
    "...     urls = []\n",
    "...     for page in range(start_page, end_page):\n",
    "...         urls.append(url % page)\n",
    "...     return urls\n",
    "...\n",
    ">>> generate_urls(URL, START_PAGE, END_PAGE)\n",
    "['http://www.majortests.com/gre/wordlist_01', 'http://www.majortests.com/gre/wordlist_02', 'http://www.majortests.com/gre/wordlist_03', 'http://www.majortests.com/gre/wordlist_04', 'http://www.majortests.com/gre/wordlist_05', 'http://www.majortests.com/gre/wordlist_06', 'http://www.majortests.com/gre/wordlist_07', 'http://www.majortests.com/gre/wordlist_08', 'http://www.majortests.com/gre/wordlist_09']\n",
    "USING AN HTTP CLIENT\n",
    "We will use the requests module as an HTTP client to get the web resources:\n",
    "\n",
    ">>> import requests\n",
    ">>> def get_resource(url):\n",
    "...     return requests.get(url)\n",
    "...\n",
    ">>> get_resource(\"http://www.majortests.com/gre/wordlist_01\")\n",
    "<Response [200]>\n",
    "In the preceding code, the get_resource function takes url as an argument and uses the requests module to get the \n",
    "resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Building a web scraping bot – a practical example\n",
    "At this point of time, our minds got enlightened with all sorts of clues to scrape the Web. With all the information acquired, let's look at a practical example. Now, we will create a web scraping bot, which will pull a list of words from a web resource and store them in a JSON file.\n",
    "\n",
    "Let's turn on the scraping mode!\n",
    "\n",
    "The web scraping bot\n",
    "Here, the web scraping bot is an automated script that has the capability to extract words from a website named majortests.com. This website consists of various tests and Graduate Record Examinations (GRE) word lists. With this web scraping bot, we will scrape the previously mentioned website and create a list of GRE words and their meanings in a JSON file.\n",
    "\n",
    "The following image is the sample page of the website that we are going to scrape:\n",
    "\n",
    "The web scraping bot\n",
    "Before we kick start the scraping process, let's revise the dos and don't of web scraping as mentioned in the initial part of the chapter. Believe it or not they will definitely leave us in peace:\n",
    "\n",
    "Do refer to the terms and conditions: Yes, before scraping majortests.com, refer to the terms and conditions of the site and obtain the necessary legal permissions to scrape it.\n",
    "Don't bombard the server with a lot of requests: Keeping this in mind, for every request that we are going to send to the website, a delay has been instilled using Python's time.sleep function.\n",
    "Do track the web resource from time to time: We ensured that the code runs perfectly with the website that is running on the server. Do check the site once before starting to scrape, so that it won't break the code. This can be made possible by running some unit tests, which conform to the structure we expected.\n",
    "Now, let's start the implementation by following the steps to scrape that we discussed previously.\n",
    "\n",
    "IDENTIFYING THE URL OR URLS\n",
    "The first step in web scraping is to identify the URL or a list of URLs that will result in the required resources. In this case, our intent is to find all the URLs that result in the expected list of GRE words. The following is the list of the URLs of the sites that we are going to scrape:\n",
    "\n",
    "http://www.majortests.com/gre/wordlist_01,\n",
    "\n",
    "http://www.majortests.com/gre/wordlist_02,\n",
    "\n",
    "http://www.majortests.com/gre/wordlist_03, and so on\n",
    "\n",
    "Our aim is to scrape words from nine such URLs, for which we found a common pattern. This will help us to crawl all of them. The common URL pattern for all those URLs is written using Python's string object, as follows:\n",
    "\n",
    "http://www.majortests.com/gre/wordlist_0%d\n",
    "\n",
    "In our implementation, we defined a method called generate_urls, which will generate the required list of URLs using the preceding URL string. The following snippet demonstrates the process in a Python shell:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> START_PAGE, END_PAGE = 1, 10\n",
    ">>> URL = \"http://www.majortests.com/gre/wordlist_0%d\"\n",
    ">>> def generate_urls(url, start_page, end_page):\n",
    "...     urls = []\n",
    "...     for page in range(start_page, end_page):\n",
    "...         urls.append(url % page)\n",
    "...     return urls\n",
    "...\n",
    ">>> generate_urls(URL, START_PAGE, END_PAGE)\n",
    "['http://www.majortests.com/gre/wordlist_01', 'http://www.majortests.com/gre/wordlist_02', 'http://www.majortests.com/gre/wordlist_03', 'http://www.majortests.com/gre/wordlist_04', 'http://www.majortests.com/gre/wordlist_05', 'http://www.majortests.com/gre/wordlist_06', 'http://www.majortests.com/gre/wordlist_07', 'http://www.majortests.com/gre/wordlist_08', 'http://www.majortests.com/gre/wordlist_09']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USING AN HTTP CLIENT\n",
    "We will use the requests module as an HTTP client to get the web resources:\n",
    "\n",
    ">>> import requests\n",
    ">>> def get_resource(url):\n",
    "...     return requests.get(url)\n",
    "...\n",
    ">>> get_resource(\"http://www.majortests.com/gre/wordlist_01\")\n",
    "<Response [200]>\n",
    "In the preceding code, the get_resource function takes url as an argument and uses the requests module to get the resource.\n",
    "\n",
    "DISCOVERING THE PIECES OF DATA TO SCRAPE\n",
    "Now, it is time to analyze and classify the contents of the web page. The content in this context is a list of words with their definitions. In order to identify the elements of the words and their definitions, we used Chrome DevTools. The perceived information of the elements (HTML elements) can help us to identify the word and its definition, which can be used in the process of scraping.\n",
    "\n",
    "To carry this out open the URL (http://www.majortests.com/gre/wordlist_01) in the Chrome browser and access the Inspect element option by right-clicking on the web page:\n",
    "\n",
    "Discovering the pieces of data to scrape\n",
    "From the preceding image, we can identify the structure of the word list, which appears in the following manner:\n",
    "\n",
    "<div class=\"grid_9 alpha\">\n",
    "  <h3>Group 1</h3>\n",
    "  <a name=\"1\"></a>\n",
    "  <table class=\"wordlist\">\n",
    "    <tbody>\n",
    "      <tr>\n",
    "        <th>Abhor</th>\n",
    "        <td>hate</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <th>Bigot</th>\n",
    "        <td>narrow-minded, prejudiced person</td>\n",
    "      </tr>\n",
    "      ...\n",
    "      ...\n",
    "    </tbody>\n",
    "  </table>\n",
    "</div>\n",
    "By looking at the parts of the previously referred to web page, we can interpret the following:\n",
    "\n",
    "Each web page consists of a word list\n",
    "Every word list has many word groups that are defined in the same div tag\n",
    "All the words in a word group are described in a table having the class attribute—wordlist\n",
    "Each and every table row (tr) in the table represents a word and its definition using the th and td tags, respectively\n",
    "UTILIZING A WEB SCRAPING TOOL\n",
    "Let's use BeautifulSoup4 as a web scraping tool to parse the obtained web page contents that we received using the requests module in one of the previous steps. By following the preceding interpretations, we can direct BeautifulSoup to access the required content of the web page and deliver it as an object:\n",
    "\n",
    "def make_soup(html_string):\n",
    "    return BeautifulSoup(html_string)\n",
    "In the preceding lines of code, the make_soup method takes the html content in the form of a string and returns a BeautifulSoup object.\n",
    "\n",
    "DRAWING THE DESIRED DATA\n",
    "The BeautifulSoup object that we obtained in the previous step is used to extract the required words and their definitions from it. Now, with the methods available in the BeautifulSoup object, we can navigate through the obtained HTML response, and then we can extract the list of words and their definitions:\n",
    "\n",
    "def get_words_from_soup(soup):\n",
    "    words = {}\n",
    "\n",
    "    for count, wordlist_table in enumerate(\n",
    "    soup.find_all(class_='wordlist')):\n",
    "\n",
    "        title = \"Group %d\" % (count + 1)\n",
    "\n",
    "        new_words = {}\n",
    "        for word_entry in wordlist_table.find_all('tr'):\n",
    "            new_words[word_entry.th.text] = word_entry.td.text\n",
    "\n",
    "        words[title] = new_words\n",
    "\n",
    "    return words\n",
    "In the preceding lines of code, get_words_from_soup takes a BeautifulSoup object and then looks for all the words contained in the wordlists class using the instance's find_all() method, and then returns a dictionary of words.\n",
    "\n",
    "The dictionary of words obtained previously will be saved in a JSON file using the following helper method:\n",
    "\n",
    "def save_as_json(data, output_file):\n",
    "    \"\"\" Writes the given data into the specified output file\"\"\"\n",
    "    with open(output_file, 'w') as outfile:\n",
    "        json.dump(data, outfile)\n",
    "On the whole, the process can be depicted in the following program:\n",
    "\n",
    "import json\n",
    "import time\n",
    "\n",
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "START_PAGE, END_PAGE, OUTPUT_FILE = 1, 10, 'words.json'\n",
    "\n",
    "# Identify the URL\n",
    "URL = \"http://www.majortests.com/gre/wordlist_0%d\"\n",
    "\n",
    "\n",
    "def generate_urls(url, start_page, end_page):\n",
    "    \"\"\"\n",
    "    This method takes a 'url' and returns a generated list of url strings\n",
    "\n",
    "        params: a 'url', 'start_page' number and 'end_page' number\n",
    "        return value: a list of generated url strings\n",
    "    \"\"\"\n",
    "    urls = []\n",
    "    for page in range(start_page, end_page):\n",
    "        urls.append(url % page)\n",
    "    return urls\n",
    "\n",
    "\n",
    "\n",
    "def get_resource(url):\n",
    "    \"\"\"\n",
    "    This method takes a 'url' and returns a 'requests.Response' object\n",
    "\n",
    "        params: a 'url'\n",
    "        return value: a 'requests.Response' object\n",
    "    \"\"\"\n",
    "    return requests.get(url)\n",
    "\n",
    "\n",
    "def make_soup(html_string):\n",
    "    \"\"\"\n",
    "    This method takes a 'html string' and returns a 'BeautifulSoup' object\n",
    "\n",
    "        params: html page contents as a string\n",
    "        return value: a 'BeautifulSoup' object\n",
    "    \"\"\"\n",
    "    return BeautifulSoup(html_string)\n",
    "\n",
    "\n",
    "def get_words_from_soup(soup):\n",
    "\n",
    "    \"\"\"\n",
    "    This method extracts word groups from a given 'BeautifulSoup' object\n",
    "\n",
    "        params: a BeautifulSoup object to extract data\n",
    "        return value: a dictionary of extracted word groups\n",
    "    \"\"\"\n",
    "\n",
    "    words = {}\n",
    "    count = 0\n",
    "\n",
    "    for wordlist_table in soup.find_all(class_='wordlist'):\n",
    "\n",
    "        count += 1\n",
    "        title = \"Group %d\" % count\n",
    "\n",
    "        new_words = {}\n",
    "        for word_entry in wordlist_table.find_all('tr'):\n",
    "            new_words[word_entry.th.text] = word_entry.td.text\n",
    "\n",
    "        words[title] = new_words\n",
    "        print \" - - Extracted words from %s\" % title\n",
    "\n",
    "    return words\n",
    "\n",
    "\n",
    "def save_as_json(data, output_file):\n",
    "    \"\"\" Writes the given data into the specified output file\"\"\"\n",
    "            json.dump(data, open(output_file, 'w'))\n",
    "\n",
    "\n",
    "def scrapper_bot(urls):\n",
    "    \"\"\"\n",
    "    Scrapper bot:\n",
    "        params: takes a list of urls\n",
    "\n",
    "        return value: a dictionary of word lists containing\n",
    "                      different word groups\n",
    "    \"\"\"\n",
    "\n",
    "    gre_words = {}\n",
    "    for url in urls:\n",
    "\n",
    "        print \"Scrapping %s\" % url.split('/')[-1]\n",
    "\n",
    "        # step 1\n",
    "\n",
    "        # get a 'url'\n",
    "\n",
    "        # step 2\n",
    "        html = requets.get(url)\n",
    "\n",
    "        # step 3\n",
    "        # identify the desired pieces of data in the url using Browser tools\n",
    "\n",
    "        #step 4\n",
    "        soup = make_soup(html.text)\n",
    "\n",
    "        # step 5\n",
    "        words = get_words_from_soup(soup)\n",
    "\n",
    "        gre_words[url.split('/')[-1]] = words\n",
    "\n",
    "        print \"sleeping for 5 seconds now\"\n",
    "        time.sleep(5)\n",
    "\n",
    "    return gre_words\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    urls = generate_urls(URL, START_PAGE, END_PAGE+1)\n",
    "\n",
    "    gre_words = scrapper_bot(urls)\n",
    "\n",
    "    save_as_json(gre_words, OUTPUT_FILE)\n",
    "Here is the content of the words.json file:\n",
    "\n",
    "{\"wordlist_04\":\n",
    "    {\"Group 10\":\n",
    "        {\"Devoured\": \"greedily eaten/consumed\",\n",
    "         \"Magnate\": \"powerful businessman\",\n",
    "         \"Cavalcade\": \"procession of vehicles\",\n",
    "         \"Extradite\": \"deport from one country back to the home...\n",
    "    .\n",
    "    .\n",
    "    .\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Summary\n",
    "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "In this chapter, you learned about different types of data that we encountered with web sources and tweaked some \n",
    "ideas. We came to know about the need for web scraping, the legal issues, and the goodies that it offers. Then, we\n",
    "jumped deep into web scraping tasks and their potential. You learned about a new library called BeautifulSoup, and\n",
    "its ins and outs, with examples.\n",
    "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "We came to know the capabilities of BeautifulSoup in depth and worked on some examples to get a clear idea on it.\n",
    "At last, we created a practical scraping bot by applying the knowledge that we gained from the previous sections,\n",
    "which enlightened us with an experience to scrape a website in real time.\n",
    "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "In the next chapter, you will learn about the Flask microframework and we will build an application using it by following the best practices."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
